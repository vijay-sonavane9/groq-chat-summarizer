{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gXgCtDDQGKD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Groq Assignment — Conversation Management & JSON Extraction\n",
        "This notebook implements **Task 1 (conversation + summarization)** and **Task 2 (JSON schema extraction via function-calling)** as required.\n",
        "Run cells in order. If you want real model calls, paste your GROQ API key when prompted in the Setup cell."
      ],
      "metadata": {
        "id": "ESGsOjm2QRQT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup: install & imports + API key prompt"
      ],
      "metadata": {
        "id": "3Yqu4zROQbgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install --quiet requests jsonschema\n",
        "\n",
        "# Imports\n",
        "import os, json, re, getpass, requests, textwrap\n",
        "from datetime import datetime\n",
        "from jsonschema import validate, ValidationError\n",
        "\n",
        "#Helper: set Groq API key safely\n",
        "def set_groq_key_interactive():\n",
        "    \"\"\"\n",
        "    Interactive prompt in Colab to set GROQ_API_KEY into os.environ for the current session.\n",
        "    If you press Enter with no input, notebook runs in MOCK mode (no network calls).\n",
        "    \"\"\"\n",
        "    existing = os.getenv(\"GROQ_API_KEY\")\n",
        "    if existing:\n",
        "        print(\"GROQ_API_KEY already set in environment (session).\")\n",
        "        return True\n",
        "    key = getpass.getpass(\"Paste your GROQ API key here (press Enter to run in MOCK mode): \").strip()\n",
        "    if key:\n",
        "        os.environ[\"GROQ_API_KEY\"] = key\n",
        "        print(\"GROQ_API_KEY set for this session.\")\n",
        "        return True\n",
        "    print(\"No key provided. Running in MOCK mode.\")\n",
        "    return False\n",
        "\n",
        "def load_key_from_file(path=\"/content/groq_key.txt\"):\n",
        "    \"\"\"\n",
        "    Optional: save your key into /content/groq_key.txt (local Colab FS) and load it.\n",
        "    Make sure you add that file to .gitignore before pushing anywhere.\n",
        "    \"\"\"\n",
        "    if os.path.exists(path):\n",
        "        with open(path, \"r\") as f:\n",
        "            k = f.read().strip()\n",
        "        if k:\n",
        "            os.environ[\"GROQ_API_KEY\"] = k\n",
        "            print(f\"Loaded GROQ_API_KEY from {path}\")\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# Call interactive setter\n",
        "set_groq_key_interactive()\n",
        "\n",
        "# we can use MOCK mode if no key\n",
        "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
        "USE_MOCK = not bool(GROQ_API_KEY)\n",
        "print(\"USE_MOCK =\", USE_MOCK)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTEtm9KeQXem",
        "outputId": "bbbd7f21-8990-4ef7-de8c-c7ae564f8dd3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paste your GROQ API key here (press Enter to run in MOCK mode): ··········\n",
            "GROQ_API_KEY set for this session.\n",
            "USE_MOCK = False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Groq OpenAI-compatible endpoint & model\n",
        "GROQ_CHAT_COMPLETIONS = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "DEFAULT_MODEL = \"openai/gpt-oss-20b\"  # We can use different different models which are vaialable\n",
        "\n",
        "HEADERS = {\"Authorization\": f\"Bearer {GROQ_API_KEY}\", \"Content-Type\": \"application/json\"} if GROQ_API_KEY else {}"
      ],
      "metadata": {
        "id": "24-BZqTJQXiC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ConversationStore class + summarization (supports mock mode)"
      ],
      "metadata": {
        "id": "z5KjX0hiRz18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConversationStore:\n",
        "    \"\"\"\n",
        "    Stores conversation messages and supports:\n",
        "    - truncation by turns / chars / words\n",
        "    - periodic summarization after every k-th add_message call\n",
        "    - mock summarizer when GROQ key not provided (safe demo)\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.messages = []   # list of dicts: {\"role\",\"content\",\"meta\",\"ts\"}\n",
        "        self.archive = []    # append-only archive if needed\n",
        "        self.counter = 0     # number of messages added (used for k-th summarization)\n",
        "\n",
        "    def add_message(self, role, content, meta=None):\n",
        "        meta = meta or {}\n",
        "        msg = {\n",
        "            \"role\": role,\n",
        "            \"content\": content,\n",
        "            \"meta\": meta,\n",
        "            \"ts\": datetime.utcnow().isoformat()\n",
        "        }\n",
        "        self.messages.append(msg)\n",
        "        self.archive.append(msg.copy())\n",
        "        self.counter += 1\n",
        "        return msg\n",
        "\n",
        "    def pretty_print(self):\n",
        "        print(f\"\\n--- Conversation ({len(self.messages)} messages) ---\")\n",
        "        for i, m in enumerate(self.messages, 1):\n",
        "            mark = \" [SUMMARY]\" if m.get(\"meta\", {}).get(\"is_summary\") else \"\"\n",
        "            print(f\"{i:02d}. {m['role'].upper():9}{mark}: {m['content']}\")\n",
        "        print(\"-----------------------------------------------\\n\")\n",
        "\n",
        "    # Truncation helpers\n",
        "    def get_history_by_turns(self, n):\n",
        "        return self.messages[-n:] if n > 0 else []\n",
        "\n",
        "    def get_history_by_chars(self, max_chars):\n",
        "        out = []\n",
        "        total = 0\n",
        "        for m in reversed(self.messages):\n",
        "            l = len(m[\"content\"])\n",
        "            if total + l > max_chars and out:\n",
        "                break\n",
        "            out.insert(0, m)\n",
        "            total += l\n",
        "        return out\n",
        "\n",
        "    def get_history_by_words(self, max_words):\n",
        "        out = []\n",
        "        total = 0\n",
        "        for m in reversed(self.messages):\n",
        "            wc = len(m[\"content\"].split())\n",
        "            if total + wc > max_words and out:\n",
        "                break\n",
        "            out.insert(0, m)\n",
        "            total += wc\n",
        "        return out\n",
        "\n",
        "    # Summarization using Groq\n",
        "    def summarize_via_groq(self, msgs, summary_words=100, max_tokens=256):\n",
        "        \"\"\"\n",
        "        If GROQ key present, call Groq (OpenAI-compatible) chat completion to get summary.\n",
        "        Otherwise, return a simple mock summary (first summary_words words).\n",
        "        \"\"\"\n",
        "        full_text = \"\\n\".join([f\"{m['role'].upper()}: {m['content']}\" for m in msgs])\n",
        "        if USE_MOCK:\n",
        "            tokens = full_text.split()\n",
        "            s = \" \".join(tokens[:summary_words])\n",
        "            return (s + (\"...\" if len(tokens) > summary_words else \"\")).strip()\n",
        "\n",
        "        prompt = (\n",
        "            f\"Summarize the conversation below into concise bullets: key facts, decisions, and action items.\\n\"\n",
        "            f\"Keep it under {summary_words} words.\\n\\nConversation:\\n{full_text}\"\n",
        "        )\n",
        "        body = {\n",
        "            \"model\": DEFAULT_MODEL,\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": \"You are a concise summarizer.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            \"max_tokens\": max_tokens,\n",
        "            \"temperature\": 0.0\n",
        "        }\n",
        "        resp = requests.post(GROQ_CHAT_COMPLETIONS, headers=HEADERS, json=body, timeout=30)\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()\n",
        "        # robust extraction of assistant text\n",
        "        try:\n",
        "            return data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "        except Exception:\n",
        "            return data.get(\"choices\", [{}])[0].get(\"text\", \"\").strip()\n",
        "\n",
        "    def maybe_summarize(self, k=3, summary_words=100, keep_tail=3):\n",
        "        \"\"\"\n",
        "        If counter % k == 0 => summarizes the *earlier* portion of history (everything except last keep_tail)\n",
        "        and replaces it with a single system summary message + the last keep_tail messages.\n",
        "        Returns the summary string if created, else None.\n",
        "        \"\"\"\n",
        "        if k <= 0 or (self.counter % k != 0):\n",
        "            return None\n",
        "        if len(self.messages) <= keep_tail:\n",
        "            return None\n",
        "\n",
        "        to_summarize = self.messages[:-keep_tail]\n",
        "        summary_text = self.summarize_via_groq(to_summarize, summary_words=summary_words)\n",
        "        summary_msg = {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"[SUMMARY] \" + summary_text,\n",
        "            \"meta\": {\"is_summary\": True},\n",
        "            \"ts\": datetime.utcnow().isoformat()\n",
        "        }\n",
        "        tail = self.messages[-keep_tail:]\n",
        "        self.messages = [summary_msg] + tail\n",
        "        return summary_text\n"
      ],
      "metadata": {
        "id": "HwBraJ9kRnhk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1 demo: feed sample conversations, show truncation & k-th summarization"
      ],
      "metadata": {
        "id": "l8wRX1HXR2AR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Demo for Task 1\n",
        "conv = ConversationStore()\n",
        "\n",
        "sample_msgs = [\n",
        "    (\"user\", \"Hi, I'm Vijay. I want to build a conversation summarizer.\"),\n",
        "    (\"assistant\", \"Great — what language and runtime are you using?\"),\n",
        "    (\"user\", \"Python on Colab for prototyping, then a VPS later.\"),\n",
        "    (\"assistant\", \"We can summarize periodically and extract structured fields from chats.\"),\n",
        "    (\"user\", \"Please ensure we preserve names, emails and action items.\"),\n",
        "    (\"assistant\", \"Sure — we'll keep entities intact and keep short bullets.\"),\n",
        "    (\"user\", \"I want summarization to run every 3rd message.\")\n",
        "]\n",
        "\n",
        "k = 3   # summarization frequency\n",
        "for role, txt in sample_msgs:\n",
        "    conv.add_message(role, txt)\n",
        "    print(f\"Added: {role} -> {txt}\")\n",
        "    summary = conv.maybe_summarize(k=k, summary_words=60, keep_tail=2)\n",
        "    if summary:\n",
        "        print(\"\\n>>> SUMMARY generated and older history replaced:\\n\", summary, \"\\n\")\n",
        "    conv.pretty_print()\n",
        "\n",
        "# Demonstrate truncation by turns, chars, and words\n",
        "print(\"Last 4 turns (truncation by turns):\")\n",
        "for m in conv.get_history_by_turns(4):\n",
        "    print(m[\"role\"].upper(), \":\", m[\"content\"])\n",
        "\n",
        "print(\"\\nHistory truncated to max 200 chars (from the end):\")\n",
        "for m in conv.get_history_by_chars(200):\n",
        "    print(m[\"role\"].upper(), \":\", m[\"content\"])\n",
        "\n",
        "print(\"\\nHistory truncated to max 50 words:\")\n",
        "for m in conv.get_history_by_words(50):\n",
        "    print(m[\"role\"].upper(), \":\", m[\"content\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSBRc2iDRnj0",
        "outputId": "a3547ad4-f9a2-41e5-965e-d5307d4fe881"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4081919897.py:19: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"ts\": datetime.utcnow().isoformat()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added: user -> Hi, I'm Vijay. I want to build a conversation summarizer.\n",
            "\n",
            "--- Conversation (1 messages) ---\n",
            "01. USER     : Hi, I'm Vijay. I want to build a conversation summarizer.\n",
            "-----------------------------------------------\n",
            "\n",
            "Added: assistant -> Great — what language and runtime are you using?\n",
            "\n",
            "--- Conversation (2 messages) ---\n",
            "01. USER     : Hi, I'm Vijay. I want to build a conversation summarizer.\n",
            "02. ASSISTANT: Great — what language and runtime are you using?\n",
            "-----------------------------------------------\n",
            "\n",
            "Added: user -> Python on Colab for prototyping, then a VPS later.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4081919897.py:110: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"ts\": datetime.utcnow().isoformat()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> SUMMARY generated and older history replaced:\n",
            " - Vijay wants to build a conversation summarizer. \n",
            "\n",
            "\n",
            "--- Conversation (3 messages) ---\n",
            "01. SYSTEM    [SUMMARY]: [SUMMARY] - Vijay wants to build a conversation summarizer.\n",
            "02. ASSISTANT: Great — what language and runtime are you using?\n",
            "03. USER     : Python on Colab for prototyping, then a VPS later.\n",
            "-----------------------------------------------\n",
            "\n",
            "Added: assistant -> We can summarize periodically and extract structured fields from chats.\n",
            "\n",
            "--- Conversation (4 messages) ---\n",
            "01. SYSTEM    [SUMMARY]: [SUMMARY] - Vijay wants to build a conversation summarizer.\n",
            "02. ASSISTANT: Great — what language and runtime are you using?\n",
            "03. USER     : Python on Colab for prototyping, then a VPS later.\n",
            "04. ASSISTANT: We can summarize periodically and extract structured fields from chats.\n",
            "-----------------------------------------------\n",
            "\n",
            "Added: user -> Please ensure we preserve names, emails and action items.\n",
            "\n",
            "--- Conversation (5 messages) ---\n",
            "01. SYSTEM    [SUMMARY]: [SUMMARY] - Vijay wants to build a conversation summarizer.\n",
            "02. ASSISTANT: Great — what language and runtime are you using?\n",
            "03. USER     : Python on Colab for prototyping, then a VPS later.\n",
            "04. ASSISTANT: We can summarize periodically and extract structured fields from chats.\n",
            "05. USER     : Please ensure we preserve names, emails and action items.\n",
            "-----------------------------------------------\n",
            "\n",
            "Added: assistant -> Sure — we'll keep entities intact and keep short bullets.\n",
            "\n",
            "--- Conversation (3 messages) ---\n",
            "01. SYSTEM    [SUMMARY]: [SUMMARY] \n",
            "02. USER     : Please ensure we preserve names, emails and action items.\n",
            "03. ASSISTANT: Sure — we'll keep entities intact and keep short bullets.\n",
            "-----------------------------------------------\n",
            "\n",
            "Added: user -> I want summarization to run every 3rd message.\n",
            "\n",
            "--- Conversation (4 messages) ---\n",
            "01. SYSTEM    [SUMMARY]: [SUMMARY] \n",
            "02. USER     : Please ensure we preserve names, emails and action items.\n",
            "03. ASSISTANT: Sure — we'll keep entities intact and keep short bullets.\n",
            "04. USER     : I want summarization to run every 3rd message.\n",
            "-----------------------------------------------\n",
            "\n",
            "Last 4 turns (truncation by turns):\n",
            "SYSTEM : [SUMMARY] \n",
            "USER : Please ensure we preserve names, emails and action items.\n",
            "ASSISTANT : Sure — we'll keep entities intact and keep short bullets.\n",
            "USER : I want summarization to run every 3rd message.\n",
            "\n",
            "History truncated to max 200 chars (from the end):\n",
            "SYSTEM : [SUMMARY] \n",
            "USER : Please ensure we preserve names, emails and action items.\n",
            "ASSISTANT : Sure — we'll keep entities intact and keep short bullets.\n",
            "USER : I want summarization to run every 3rd message.\n",
            "\n",
            "History truncated to max 50 words:\n",
            "SYSTEM : [SUMMARY] \n",
            "USER : Please ensure we preserve names, emails and action items.\n",
            "ASSISTANT : Sure — we'll keep entities intact and keep short bullets.\n",
            "USER : I want summarization to run every 3rd message.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: function schema + extraction method (supports mock fallback)"
      ],
      "metadata": {
        "id": "mQMm1GruSItD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JSON schema for extraction (using 5 fields)\n",
        "function_schema = {\n",
        "    \"name\": \"extract_info\",\n",
        "    \"description\": \"Extract name, email, phone, location and age from the message.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"name\": {\"type\": \"string\"},\n",
        "            \"email\": {\"type\": \"string\", \"format\": \"email\"},\n",
        "            \"phone\": {\"type\": \"string\"},\n",
        "            \"location\": {\"type\": \"string\"},\n",
        "            \"age\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 120}\n",
        "        },\n",
        "        \"required\": []\n",
        "    }\n",
        "}\n",
        "\n",
        "def extract_info_via_groq(text, function_schema, max_tokens=200):\n",
        "    \"\"\"\n",
        "    Uses Groq (OpenAI-compatible) chat completion with a single function definition.\n",
        "    If USE_MOCK True, returns heuristic extractions without network.\n",
        "    Returns (parsed_dict, raw_response_or_mock_metadata).\n",
        "    \"\"\"\n",
        "    if USE_MOCK:\n",
        "        parsed = {}\n",
        "        # email\n",
        "        em = re.search(r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+', text)\n",
        "        if em: parsed[\"email\"] = em.group(0)\n",
        "        # phone (very permissive)\n",
        "        ph = re.search(r'(\\+?\\d[\\d\\-\\s]{6,}\\d)', text)\n",
        "        if ph: parsed[\"phone\"] = ph.group(0)\n",
        "        # age\n",
        "        m_age = re.search(r'age\\s*(?:is|:)?\\s*(\\d{1,3})', text, flags=re.IGNORECASE)\n",
        "        if m_age:\n",
        "            try:\n",
        "                parsed[\"age\"] = int(m_age.group(1))\n",
        "            except: pass\n",
        "        # name\n",
        "        m_name = re.search(r\"(?:I am|I'm|This is|My name is)\\s+([A-Z][a-zA-Z]+(?:\\s[A-Z][a-zA-Z]+)?)\", text)\n",
        "        if m_name: parsed[\"name\"] = m_name.group(1)\n",
        "        # location\n",
        "        m_loc = re.search(r'(?:based in|live in|located in|from)\\s+([A-Za-z\\s]+)', text, flags=re.IGNORECASE)\n",
        "        if m_loc: parsed[\"location\"] = m_loc.group(1).strip().strip('.')\n",
        "        return parsed, {\"mock\": True, \"input\": text}\n",
        "\n",
        "    # Build chat + function calling body\n",
        "    body = {\n",
        "        \"model\": DEFAULT_MODEL,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a strict extractor. Use the function to emit valid JSON matching the schema.\"},\n",
        "            {\"role\": \"user\", \"content\": text}\n",
        "        ],\n",
        "        \"functions\": [function_schema],\n",
        "        \"function_call\": {\"name\": function_schema[\"name\"]},\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": 0.0\n",
        "    }\n",
        "    resp = requests.post(GROQ_CHAT_COMPLETIONS, headers=HEADERS, json=body, timeout=30)\n",
        "    resp.raise_for_status()\n",
        "    data = resp.json()\n",
        "    # Parse function_call arguments\n",
        "    parsed = {}\n",
        "    try:\n",
        "        fc = data[\"choices\"][0][\"message\"].get(\"function_call\", {})\n",
        "        args_str = fc.get(\"arguments\")\n",
        "        if args_str:\n",
        "            parsed = json.loads(args_str)\n",
        "        else:\n",
        "            # fallback: maybe assistant content is JSON\n",
        "            content = data[\"choices\"][0][\"message\"].get(\"content\", \"\")\n",
        "            parsed = json.loads(content) if content.strip().startswith(\"{\") else {}\n",
        "    except Exception:\n",
        "        parsed = {}\n",
        "    return parsed, data\n",
        "\n",
        "def validate_against_schema(obj, schema):\n",
        "    try:\n",
        "        validate(instance=obj, schema=schema)\n",
        "        return True, None\n",
        "    except ValidationError as e:\n",
        "        return False, str(e)\n"
      ],
      "metadata": {
        "id": "-lnvIE82RnmM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task 2 demo: parse with3 sample chats and validate"
      ],
      "metadata": {
        "id": "tbwIBD2-Sa2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_chats = [\n",
        "    \"Hi, I'm Rahul Sharma. My email is rahul.sharma@example.com. Phone: +91-9876543XXX live in Pune. I'm 23 years old.\",\n",
        "    \"This is Neha — reach me at neha.work@gmail.com. Based in Bengaluru. Age: 28. Call me at 98765123XX.\",\n",
        "    \"Hey! Name: Arjun Patel. Contact: arjun.patel@domain.co. Location - Mumbai. Age: 25. Mobile +91 91234 567XX.\"\n",
        "]\n",
        "\n",
        "for i, s in enumerate(sample_chats, 1):\n",
        "    print(f\"\\n--- Sample {i} ---\\n{s}\\n\")\n",
        "    parsed, raw = extract_info_via_groq(s, function_schema)\n",
        "    print(\"Parsed:\", parsed)\n",
        "    valid, err = validate_against_schema(parsed, function_schema[\"parameters\"])\n",
        "    if valid:\n",
        "        print(\"Validation: PASSED ✅\")\n",
        "    else:\n",
        "        print(\"Validation: FAILED ❌\")\n",
        "        print(\"Validation error:\", err)\n",
        "    if not USE_MOCK:\n",
        "        # show minimal raw keys for debugging\n",
        "        print(\"Raw response keys:\", list(raw.keys()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NabnH8vzRnpl",
        "outputId": "6c74d5d7-9f95-4d43-bd8b-820fadaae766"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sample 1 ---\n",
            "Hi, I'm Rahul Sharma. My email is rahul.sharma@example.com. Phone: +91-9876543XXX live in Pune. I'm 23 years old.\n",
            "\n",
            "Parsed: {'age': 23, 'email': 'rahul.sharma@example.com', 'location': 'Pune', 'name': 'Rahul Sharma', 'phone': '+91-9876543XXX'}\n",
            "Validation: PASSED ✅\n",
            "Raw response keys: ['id', 'object', 'created', 'model', 'choices', 'usage', 'usage_breakdown', 'system_fingerprint', 'x_groq', 'service_tier']\n",
            "\n",
            "--- Sample 2 ---\n",
            "This is Neha — reach me at neha.work@gmail.com. Based in Bengaluru. Age: 28. Call me at 98765123XX.\n",
            "\n",
            "Parsed: {'age': 28, 'email': 'neha.work@gmail.com', 'location': 'Bengaluru', 'name': 'Neha', 'phone': '98765123XX'}\n",
            "Validation: PASSED ✅\n",
            "Raw response keys: ['id', 'object', 'created', 'model', 'choices', 'usage', 'usage_breakdown', 'system_fingerprint', 'x_groq', 'service_tier']\n",
            "\n",
            "--- Sample 3 ---\n",
            "Hey! Name: Arjun Patel. Contact: arjun.patel@domain.co. Location - Mumbai. Age: 25. Mobile +91 91234 567XX.\n",
            "\n",
            "Parsed: {'age': 25, 'email': 'arjun.patel@domain.co', 'location': 'Mumbai', 'name': 'Arjun Patel', 'phone': '+91 91234 567XX'}\n",
            "Validation: PASSED ✅\n",
            "Raw response keys: ['id', 'object', 'created', 'model', 'choices', 'usage', 'usage_breakdown', 'system_fingerprint', 'x_groq', 'service_tier']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OTXWrU253bUQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dOHENM39StdI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GitHub Info\n",
        "\n",
        "This notebook implements both tasks of the Groq Internship Assignment:\n",
        "\n",
        "\n",
        "*   Task 1: Conversation management with summarization  \n",
        "*   Task 2: JSON schema classification & structured extraction\n",
        "\n",
        "\n",
        "\n",
        "### Repo Setup\n",
        "1. Created GitHub repo: https://github.com/vijay-sonavane9/groq-chat-summarizer\n",
        "2. Added `.gitignore` to prevent committing API keys:\n",
        "   ```text\n",
        "   *.env\n",
        "   api_key.txt\n",
        "   .ipynb_checkpoints/\n"
      ],
      "metadata": {
        "id": "8vrYMmTD63QJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Y0ol--_SphT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4LRdK1P6Spk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7gM04a37QRTr"
      }
    }
  ]
}